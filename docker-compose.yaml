version: "3"

services:
  # Minio container to mimic AWS S3 service
  minio:
    restart: always
    image: minio/minio:RELEASE.2023-07-21T21-12-44Z
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ':9001' --address ':9000'
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=minio123
    volumes:
      - ./data/s3:/data
    networks:
      - net
    profiles:
      ["ingestion", "training-manual", "training-workflow", "simulation", "all"]

  minio-init:
    image: minio/mc:RELEASE.2023-07-21T20-44-27Z
    depends_on:
      - minio
    env_file:
      - .env.local
    entrypoint: >
      /bin/sh -c "
      /tmp/wait-for-it.sh minio:9000 &&
      /usr/bin/mc alias set minio http://minio:9000 minio minio123 &&
      /usr/bin/mc mb --ignore-existing minio/mlflow-bucket &&
      /usr/bin/mc mb --ignore-existing minio/accident-severity-workflow-data;
      exit 0;
      "
    volumes:
      - ./wait-for-it.sh:/tmp/wait-for-it.sh
    networks:
      - net
    profiles:
      [
        "ingestion",
        "training-manual",
        "training-workflow",
        "workflow-orchestration",
        "simulation",
        "all",
      ]

  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  dagster_storage:
    image: postgres:13
    environment:
      POSTGRES_USER: "postgres_user"
      POSTGRES_PASSWORD: "postgres_password"
      POSTGRES_DB: "postgres_db"
    volumes:
      - dagster-storage-volume:/var/lib/postgresql/data
    networks:
      - net
    profiles: ["ingestion", "training-workflow", "simulation", "all"]

  # This service runs dagit, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from dagit will be put on
  # a queue and later dequeued and launched by dagster-daemon.
  dagster_dagit:
    build:
      context: workflow-orchestration
      dockerfile: Dockerfile
    entrypoint:
      - dagit
      - -h
      - "0.0.0.0"
      - -p
      - "3000"
      - -w
      - workspace.yaml
    expose:
      - "3000"
    ports:
      - "3000:3000"
    env_file:
      - .env.local
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      DATA_DIR: "/opt/dagster/dagster_home/storage"
      CONFIG_DIR: "/opt/dagster/dagster_home/workflows/config"
    volumes: # Make docker client accessible so we can terminate containers from dagit
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - "./data:/opt/dagster/dagster_home/storage"
      - "./notebooks:/opt/dagster/dagster_home/notebooks"
    networks:
      - net
    depends_on:
      - dagster_storage
      - minio
    profiles: ["ingestion", "training-workflow", "simulation", "all"]

  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  dagster_daemon:
    build:
      context: workflow-orchestration
      dockerfile: Dockerfile
    entrypoint:
      - dagster-daemon
      - run
    restart: on-failure
    env_file:
      - .env.local
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      DATA_DIR: "/opt/dagster/dagster_home/storage"
      CONFIG_DIR: "/opt/dagster/dagster_home/workflows/config"
    volumes: # Make docker client accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - "./data:/opt/dagster/dagster_home/storage"
      - "./notebooks:/opt/dagster/dagster_home/notebooks"
    networks:
      - net
    depends_on:
      - dagster_storage
      - minio
    profiles: ["ingestion", "training-workflow", "simulation", "all"]

  training-jupyterlab:
    build:
      context: model-training
      dockerfile: Dockerfile.jupyter
    ports:
      - 8888:8888
    volumes:
      - "./data:/home/jovyan/data"
      - "./config:/home/jovyan/config"
      - "./notebooks:/home/jovyan/work"
      - "./model-training:/home/jovyan/code"
    env_file:
      - .env.local
    environment:
      - PYTHONPATH=$PYTHONPATH:/home/jovyan/code/
      - DATA_DIR=/home/jovyan/data
      - STORAGE_DIR=/home/jovyan/data/storage
      - CONFIG_DIR=/home/jovyan/config
      - JUPYTER_ENABLE_LAB=yes
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    entrypoint: start-notebook.sh --NotebookApp.token="" --NotebookApp.notebook_dir=/home/jovyan/work
    networks:
      - net
    profiles: ["training-manual", "all"]

  mlflow_db:
    restart: always
    image: mysql/mysql-server:8.0
    environment:
      - MYSQL_DATABASE=mysql_db
      - MYSQL_USER=mysql_user
      - MYSQL_PASSWORD=mysql_pass
      - MYSQL_ROOT_PASSWORD=mysql_root
    networks:
      - net
    volumes:
      - mlflow_dbdata:/var/lib/mysql

  mlflow:
    build:
      context: model-training
      dockerfile: Dockerfile.mlflow
    entrypoint:
      - mlflow
      - server
      - --backend-store-uri
      - mysql+pymysql://mysql_user:mysql_pass@mlflow_db:3306/mysql_db
      - --default-artifact-root
      - s3://mlflow-bucket/
      - --host
      - 0.0.0.0
    env_file:
      - .env.local
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    ports:
      - 5000:5000
    networks:
      - net
    depends_on:
      - minio
    profiles: ["training-manual", "training-workflow", "simulation", "all"]

  training-dagster:
    build:
      context: model-training
      dockerfile: Dockerfile.dagster
    entrypoint:
      - dagster
      - api
      - grpc
      - -h
      - "0.0.0.0"
      - -p
      - "4000"
      - -m
      - workflows.definitions
    ports:
      - "4000:4000"
    volumes:
      - "./model-training:/opt/dagster/app"
      - "./notebooks:/opt/dagster/notebooks"
    env_file:
      - .env.local
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      STORAGE_DIR: "/opt/dagster/dagster_home/storage"
      DATA_DIR: "/home/jovyan/data"
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      MLFLOW_S3_ENDPOINT_URL: "http://minio:9000"
    networks:
      - net
    profiles: ["training-workflow", "all"]

  deployed-model:
    build:
      context: model-deployment
      dockerfile: Dockerfile
    image: accident-severity-model:$MODEL_VERSION
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_REGION: eu-central-1
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      MODEL_NAME: accident-severity
      MODEL_VERSION: $MODEL_VERSION
    depends_on:
      - mlflow
    command: /start.sh
    ports:
      - "8000:8000"
    networks:
      - net
    profiles: ["simulation", "all"]

  model-monitoring:
    build:
      context: model-monitoring
      dockerfile: Dockerfile.dagster
    command: >
      /bin/bash -c "
      export INITIAL_UNIX_TIMESTAMP=$$(date +%s) &&
      python3 init_scripts.py &&
      dagster api grpc -h 0.0.0.0 -p 4000 -m workflows.definitions
      "
    ports:
      - "4000:4000"
    volumes:
      - "./model-monitoring:/opt/dagster/app"
    env_file:
      - .env.local
    depends_on:
      - inference_db
    environment:
      DAGSTER_POSTGRES_USER: postgres_user
      DAGSTER_POSTGRES_PASSWORD: postgres_password
      DAGSTER_POSTGRES_DB: postgres_db
      SECONDS_PER_DAY: $SECONDS_PER_DAY
      POSTGRES_INFERENCE_SERVER: inference_db
      POSTGRES_INFERENCE_DB: db
      POSTGRES_INFERENCE_USER: postgres_user
      POSTGRES_INFERENCE_PASSWORD: postgres_password
      POSTGRES_INFERENCE_PORT: 5432
      PREDICT_URL: http://deployed-model:8000/predict
    networks:
      - net
    profiles: ["simulation", "all"]

  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  inference_db:
    image: postgres:13
    environment:
      POSTGRES_USER: postgres_user
      POSTGRES_PASSWORD: postgres_password
      POSTGRES_DB: db
    networks:
      - net
    volumes:
      - inference-db-volume:/var/lib/postgresql/data
    profiles: ["simulation", "all"]

  adminer:
    image: adminer:4.8.1
    restart: always
    environment:
      ADMINER_DEFAULT_SERVER: inference_db
    ports:
      - "8080:8080"
    networks:
      - net
    depends_on:
      - inference_db
    profiles: ["simulation", "all"]

  grafana:
    image: grafana/grafana:10.0.3
    user: "472"
    ports:
      - "3030:3000"
    volumes:
      - "./model-monitoring/config/grafana_datasources.yaml:/etc/grafana/provisioning/datasources/datasource.yaml:ro"
      - "./model-monitoring/config/grafana_dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml:ro"
      - "./model-monitoring/dashboards:/opt/grafana/dashboards"
      - "./model-monitoring/config/grafana.ini:/etc/grafana/grafana.ini:ro"
    networks:
      - net
    restart: always
    profiles: ["simulation", "all"]

networks:
  net:
    name: net
    driver: bridge

volumes:
  dagster-storage-volume:
  mlflow_dbdata:
  inference-db-volume:
