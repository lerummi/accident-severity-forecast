version: "3"

services:
  # Localstack container to mimic AWS services
  localstack:
    image: localstack/localstack:2.2
    ports:
      - "4566:4566" # LocalStack endpoint
      - "4510-4559:4510-4559" # external services port range
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - ./localstack-init.sh:/etc/localstack/init/ready.d/script.sh
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      - net
    profiles: ["model-training", "workflow-orchestration", "all"]

  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  dagster_storage:
    image: postgres:13
    environment:
      POSTGRES_USER: "postgres_user"
      POSTGRES_PASSWORD: "postgres_password"
      POSTGRES_DB: "postgres_db"
    networks:
      - net
    profiles: ["workflow-orchestration", "all"]

  # This service runs dagit, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from dagit will be put on
  # a queue and later dequeued and launched by dagster-daemon.
  dagster_dagit:
    build:
      context: workflow-orchestration
      dockerfile: Dockerfile
    entrypoint:
      - dagit
      - -h
      - "0.0.0.0"
      - -p
      - "3000"
      - -w
      - workspace.yaml
    expose:
      - "3000"
    ports:
      - "3000:3000"
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      DATA_DIR: "/opt/dagster/dagster_home/storage"
      CONFIG_DIR: "/opt/dagster/dagster_home/workflows/config"
    volumes: # Make docker client accessible so we can terminate containers from dagit
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - "./data:/opt/dagster/dagster_home/storage"
      - "./notebooks:/opt/dagster/dagster_home/notebooks"
    networks:
      - net
    depends_on:
      - dagster_storage
    profiles: ["workflow-orchestration", "all"]

  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  dagster_daemon:
    build:
      context: workflow-orchestration
      dockerfile: Dockerfile
    entrypoint:
      - dagster-daemon
      - run
    restart: on-failure
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      DATA_DIR: "/opt/dagster/dagster_home/storage"
      CONFIG_DIR: "/opt/dagster/dagster_home/workflows/config"
    volumes: # Make docker client accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - "./data:/opt/dagster/dagster_home/storage"
      - "./notebooks:/opt/dagster/dagster_home/notebooks"
    networks:
      - net
    depends_on:
      - dagster_storage
    profiles: ["workflow-orchestration", "all"]

  training-jupyterlab:
    build:
      context: model-training
      dockerfile: Dockerfile.jupyter
    ports:
      - 8888:8888
    volumes:
      - "./data:/home/jovyan/data"
      - "./config:/home/jovyan/config"
      - "./model-tracking-registry/mlartifacts:/mlartifacts"
      - "./notebooks:/home/jovyan/work"
      - "./model-training:/home/jovyan/code"
    env_file:
      - .env.local
    environment:
      - PYTHONPATH=$PYTHONPATH:/home/jovyan/code/
      - DATA_DIR=/home/jovyan/data
      - STORAGE_DIR=/home/jovyan/data/storage
      - CONFIG_DIR=/home/jovyan/config
      - JUPYTER_ENABLE_LAB=yes
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    entrypoint: start-notebook.sh --NotebookApp.token="" --NotebookApp.notebook_dir=/home/jovyan/work
    networks:
      - net
    profiles: ["model-training", "all"]

  mlflow:
    build:
      context: model-tracking-registry
      dockerfile: Dockerfile
    entrypoint:
      - mlflow
      - server
      - --default-artifact-root
      - s3://mlflow-bucket/
      - --host
      - 0.0.0.0
    env_file:
      - .env.local
    ports:
      - 5000:5000
    networks:
      - net
    depends_on:
      - localstack
    profiles: ["model-training", "workflow-orchestration", "all"]

  training-dagster:
    build:
      context: model-training
      dockerfile: Dockerfile.dagster
    entrypoint:
      - dagster
      - api
      - grpc
      - -h
      - "0.0.0.0"
      - -p
      - "4000"
      - -m
      - workflows.definitions
    ports:
      - "4000:4000"
    volumes:
      - "./model-training:/opt/dagster/app"
      - "./notebooks:/opt/dagster/notebooks"
      - "./data:/home/jovyan/data"
      - "./data/assets:/opt/dagster/dagster_home/storage"
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      STORAGE_DIR: "/opt/dagster/dagster_home/storage"
      DATA_DIR: "/home/jovyan/data"
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
    networks:
      - net
    profiles: ["workflow-orchestration", "all"]

networks:
  net:
    name: net
    driver: bridge
